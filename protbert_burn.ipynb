{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cathepsin/alpha_beta_ProtBERT/blob/main/protbert_burn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY4KoYGCKxGn"
      },
      "source": [
        "#MM-BERT MLM Predictions With ProtBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings upload\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "Settings_source = \"From Computer\" #@param {values: [\"From Computer\", \"From Drive or Colab\"]}\n",
        "#@markdown If using a settings file from Drive or Colab, please specify the filepath below\n",
        "Settings_filepath_in_drive_or_colab = \"\" #@param {type: \"string\"}\n",
        "#@markdown Prompt to redownload a new settings file every time this program is run. Otherwise if a settings file is detected then it will be reused.\n",
        "Redownload_settings = False #@param {type : \"boolean\"}\n",
        "\n",
        "if Redownload_settings == False and 'all_settings' in globals():\n",
        "  pass\n",
        "elif Settings_source == \"From Computer\":\n",
        "  filepath = next(iter(files.upload().keys()))\n",
        "else:\n",
        "  filepath = Settings_filepath_in_drive_or_colab\n",
        "\n",
        "with open(filepath, \"r\") as fp:\n",
        "  all_settings = json.loads(fp.read())\n",
        "  configuration = all_settings[\"Configuration_Settings\"]\n",
        "  finetune = all_settings[\"Fine_Tune_Settings\"]\n",
        "  mlm = all_settings[\"MLM_Settings\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "05qwNQwWS6yZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8cBGIN4YeQMX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup, Imports, and Connect to Runtime\n",
        "!pip install -q transformers\n",
        "if  \"completed_setup\" not in globals():\n",
        "  from termcolor import colored\n",
        "  from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
        "  from transformers import AdamW\n",
        "  import os\n",
        "  import torch\n",
        "  from tqdm import tqdm\n",
        "  from time import gmtime, strftime\n",
        "  import re\n",
        "  import copy as cp\n",
        "  import sys\n",
        "  import random\n",
        "  import matplotlib.pyplot as plt\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "  tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')\n",
        "  model = BertForMaskedLM.from_pretrained('Rostlab/prot_bert')\n",
        "  completed_setup = True\n",
        "\n",
        "use_custom_weights = configuration['use_custom_weights']\n",
        "path_to_file_in_colab = configuration['path_to_file_in_colab']\n",
        "fine_tune_model = configuration['fine_tune_model']\n",
        "MLM_model = configuration['MLM_prediction']\n",
        "\n",
        "if use_custom_weights == 'From Google Drive or Colab':\n",
        "  #Make sure paths exist and are valid\n",
        "  assert path_to_file_in_colab, 'No path given'\n",
        "  assert os.path.isfile(path_to_file_in_colab), 'Invalid filepath'\n",
        "  model.load_state_dict(torch.load(path_to_file_in_colab))\n",
        "elif use_custom_weights == 'From computer':\n",
        "  #prompt user to upload weight file\n",
        "  model.load_state_dict(torch.load(files.upload()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dnQG1nZQn4FC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Fine-tune model\n",
        "fsettings = finetune['FT_Settings']\n",
        "ssettings = finetune['Save_Settings']\n",
        "\n",
        "fine_file = fsettings['fine_tune_file_source']\n",
        "path_to_ft_file = fsettings['path_to_fine_tune_file']\n",
        "percent_mask = fsettings['percent_to_mask']\n",
        "max_len = fsettings['max_length']\n",
        "batch_size = fsettings['max_length']\n",
        "number_of_epochs = fsettings['number_of_epochs']\n",
        "learning_rate = fsettings['learning_rate']\n",
        "\n",
        "save_every = ssettings['save_every_number_of_steps']\n",
        "save_to_drive = ssettings['save_final_to_drive']\n",
        "download_to_computer = ssettings['download_final_state']\n",
        "\n",
        "if save_to_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "def RunningAvg(lst):\n",
        "  ret = []\n",
        "  for i in range(len(lst)):\n",
        "    try:\n",
        "      ret.append(sum(lst[0:i])/len(lst[0:i]))\n",
        "    except ZeroDivisionError:\n",
        "      ret.append(0)\n",
        "  return ret\n",
        "\n",
        "def GetAccuracy(input_id, labels, logits, mask_label = 4):\n",
        "  num_correct = 0\n",
        "  total = 0\n",
        "  for i in range(len(input_id)):\n",
        "    for v in input_id[i]:\n",
        "      if input_id[i][v] == mask_label:\n",
        "        total += 1\n",
        "        val = torch.argmax(torch.softmax(logits[i][v], -1))\n",
        "        if labels[i][v] == val:\n",
        "          num_correct += 1\n",
        "  return num_correct / total\n",
        "\n",
        "class ProtDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return {key: val[index].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "if fine_tune_model:\n",
        "  if fine_file == 'From Google Drive or Colab':\n",
        "    fp = path_to_ft_file\n",
        "  else:\n",
        "    up = files.upload()\n",
        "    fp = list(up.keys())[0]\n",
        "    up.clear()\n",
        "\n",
        "  with open(fp, 'r') as f:\n",
        "    text = f.read().split('\\n')\n",
        "    for i in range(len(text)):\n",
        "      text[i] = \" \".join(text[i])\n",
        "\n",
        "  print(\"Tokenizing file...\")\n",
        "  inputs = tokenizer(text, return_tensors='pt', max_length=max_len, truncation=True, padding='max_length')\n",
        "  inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "  rand = torch.rand(inputs.input_ids.shape)\n",
        "  mask_ar = (rand < percent_mask) * (inputs.input_ids != 2) * (inputs.input_ids != 0) * (inputs.input_ids != 3)\n",
        "\n",
        "  selection = []\n",
        "  for i in range(mask_ar.shape[0]):\n",
        "    selection.append(torch.flatten(mask_ar[i].nonzero()).tolist())\n",
        "\n",
        "  for i in range(mask_ar.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = 4\n",
        "\n",
        "  dataset = ProtDataset(inputs)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  if not torch.cuda.is_available():\n",
        "    print(\"No CUDA enabled GPU available. Running on CPU\")\n",
        "\n",
        "  model.to(device);\n",
        "  model.train();\n",
        "\n",
        "  optim = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  cnt = 1\n",
        "  graph_count = []\n",
        "  graph_loss = []\n",
        "  graph_accuracies = []\n",
        "  # nan = open('nan.txt', 'w')\n",
        "  # debug = open('debug.txt', 'w')\n",
        "  for epoch in range(number_of_epochs):\n",
        "    loop = tqdm(dataloader, position=0, leave=True)\n",
        "    for batch in loop:\n",
        "      optim.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      graph_count.append(cnt)\n",
        "      graph_loss.append(loss.item())\n",
        "\n",
        "      graph_accuracies.append(GetAccuracy(input_ids, labels, outputs['logits']))\n",
        "      # debug.write(f\"{cnt} {graph_accuracies[-1]} {loss}\\n\")\n",
        "      # debug.write(f\"\\t{outputs['logits']}\\n\")\n",
        "      # for l in outputs['logits']:\n",
        "      #   for v in l:\n",
        "      #     if True in torch.isnan(v):\n",
        "      #       print(f\"Nan value in {cnt}\")\n",
        "\n",
        "      # if cnt == 10:\n",
        "      #   debug.close()\n",
        "      #   raise\n",
        "      # if cnt % save_every == 0:\n",
        "      #   torch.save(model.state_dict(), '/content/save_state')\n",
        "      #   if save_recent_state_to_drive:\n",
        "      #     torch.save(model.state_dict(), path_to_most_recent + \"/most_recent\")\n",
        "\n",
        "      #####################\n",
        "           ####BETA####\n",
        "      # if cnt % smart_steps == 0 and smart_learn and cnt != 0:\n",
        "      #   avg = sum(graph_loss[-smart_steps:]) / smart_steps\n",
        "      #   for l in optim.param_groups:\n",
        "      #     l['lr'] /= (1 + avg)\n",
        "      #####################\n",
        "      cnt += 1\n",
        "      loop.set_description(f'Epoch: {epoch}, Step: {cnt}, Accuracy: {graph_accuracies[-1]:.3f}, Loss: {loss.item():.8f}')\n",
        "      loop.update(1)\n",
        "\n",
        "  # debug.close()\n",
        "  plt.plot(graph_count, graph_loss)\n",
        "  plt.plot(graph_count, RunningAvg(graph_loss))\n",
        "  plt.grid()\n",
        "  plt.title(\"Fine-Tuning Loss\")\n",
        "  plt.xlabel(\"Step\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(graph_count, graph_accuracies)\n",
        "  plt.grid()\n",
        "  plt.title(\"Fine-Tuning Accuracy\")\n",
        "  plt.xlabel(\"Step\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "\n",
        "\n",
        "  plt.figure()\n",
        "  plt.title(\"Combined Graph\")\n",
        "  plt.xlabel(\"Step\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.grid()\n",
        "  plt2 = plt.twinx()\n",
        "  plt.plot(graph_count, graph_loss)\n",
        "  plt2.plot(graph_count, graph_accuracies)\n",
        "  plt2.set_ylabel(\"Accuracy\")\n",
        "  plt2.set_ylim([0.0, 1.0])\n",
        "\n",
        "  plt.legend([\"Loss\", \"Accuracy\"])\n",
        "  torch.save(model.state_dict(), '/content/save_state_final')\n",
        "  if save_to_drive:\n",
        "    print(\"Saving to google drive is currently disabled. Please manually move the save state called 'save_state-final' to your drive\")\n",
        "    # from google.colab import drive\n",
        "    # os.system(r'mv /content/save ' + colab_save_path)\n",
        "  if download_to_computer:\n",
        "    torch.save(model.state_dict(), '/content/save_state')\n",
        "    files.download('/content/save_state')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fELedsNluF09"
      },
      "outputs": [],
      "source": [
        "#@title Predictions\n",
        "###\n",
        "seqsettings = mlm['Sequence_Settings']\n",
        "asettings = mlm['Acceptance_Settings']\n",
        "bsettings = mlm['Burn-in_Settings']\n",
        "rsettings = mlm['Results_and_Display_Settings']\n",
        "lsettings = mlm['Logging']\n",
        "\n",
        "write_log = lsettings['write_log_file']\n",
        "num_to_log = lsettings['number_to_log']\n",
        "\n",
        "order = rsettings['order']\n",
        "get_score_average = rsettings['get_score_average']\n",
        "get_score_entropy = rsettings['get_score_entropy']\n",
        "spaces_in_output = rsettings['spaces_in_output']\n",
        "display_iterations = rsettings['display_iterations']\n",
        "display_num_different = rsettings['display_num_different']\n",
        "display_score_average = rsettings['display_score_average']\n",
        "display_score_entropy = rsettings['display_score_entropy']\n",
        "\n",
        "use_random_sequence = seqsettings['use_random_sequence']\n",
        "random_sequence_length = seqsettings['random_sequence_length']\n",
        "original = seqsettings['original_sequence']\n",
        "masked = seqsettings['masked_sequence  ']\n",
        "match_original = seqsettings['mask_all_residues']\n",
        "process_batch = seqsettings['process_batch']\n",
        "path_to_batch = seqsettings['path_to_batch']\n",
        "\n",
        "deterministic = asettings['deterministic']\n",
        "seed = asettings['deterministic_seed']\n",
        "slowdown = asettings['slowdown']\n",
        "use_threshold = asettings['use_threshold']\n",
        "number_of_iterations = asettings['max_num_iterations']\n",
        "num_changes_to_keep = asettings['num_changes_to_keep']\n",
        "keep_threshold_percent = asettings['threshold_percent']\n",
        "\n",
        "burn_in = bsettings['iterations_to_burnin']\n",
        "burn_percent = bsettings['burn_threshold']\n",
        "burn_num_to_keep = bsettings['burn_num_changes_to_keep']\n",
        "\n",
        "import math\n",
        "from IPython.utils.text import num_ini_spaces\n",
        "import os\n",
        "\n",
        "bi = burn_in\n",
        "\n",
        "if deterministic:\n",
        "  random.seed(seed)\n",
        "\n",
        "AAs = \"ARNDCEQGHILKMFPSTWYV\"\n",
        "\n",
        "original = original.strip()\n",
        "\n",
        "def FindAllChar(str, char):\n",
        "    lst = []\n",
        "    for i in range(len(str)):\n",
        "        if str[i] == char:\n",
        "            lst.append(i)\n",
        "    return lst\n",
        "\n",
        "def CopyList(lst):\n",
        "  ret = []\n",
        "  for val in lst:\n",
        "    ret.append(val)\n",
        "  return ret\n",
        "\n",
        "def ListIsEqual(l1, l2):\n",
        "  if len(l1) != len(l2):\n",
        "    return False\n",
        "  for i in range(len(l1)):\n",
        "    if l1[i] != l2[i]:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def Update(attempts, sequence, num_to_change = None, threshold = 0.0 ):\n",
        "  if num_to_change is None or num_to_change == -1:\n",
        "    num_to_change = len(attempts)\n",
        "  attempts.sort(key = lambda x: x[1]['score'])\n",
        "  attempts.reverse()\n",
        "  updates = 0\n",
        "  i = 0\n",
        "  while i < len(attempts) and updates < num_to_change:\n",
        "    index = attempts[i][0]\n",
        "    if attempts[i][1]['sequence'].split()[index] != sequence[index] and attempts[i][1]['score'] > threshold:\n",
        "      sequence[index] = attempts[i][1]['sequence'].split()[index]\n",
        "      updates += 1\n",
        "    i += 1\n",
        "    if updates == num_to_change:\n",
        "      return sequence, updates\n",
        "  return sequence, updates\n",
        "\n",
        "def Normalize(probabilities):\n",
        "  factor = 1 / sum(probabilities)\n",
        "  return [p*factor for p in probabilities]\n",
        "\n",
        "def UpdateNoThresh(attempts, sequence, num_to_change=None, slow=False):\n",
        "  updates = 0\n",
        "  if not slow:\n",
        "    for a in attempts:\n",
        "      probabilities = [x[1]['score'] for x in a]\n",
        "      chosen = random.choices(a, weights=probabilities, k=1)[0]\n",
        "      old_char = sequence[chosen[0]]\n",
        "      new_char = chosen[1]['sequence'].split()[chosen[0]]\n",
        "      sequence[chosen[0]] = new_char\n",
        "      if old_char != new_char:\n",
        "        updates += 1\n",
        "  else:\n",
        "    flat = [a for attempt in attempts for a in attempt]\n",
        "    probabilities = [x[1]['score'] for x in flat]\n",
        "    normalized = Normalize(probabilities)\n",
        "    choices = random.choices(flat, weights=normalized, k=num_to_change)\n",
        "    for chosen in choices:\n",
        "      old_char = sequence[chosen[0]]\n",
        "      new_char = chosen[1]['sequence'].split()[chosen[0]]\n",
        "      sequence[chosen[0]] = new_char\n",
        "      if old_char != new_char:\n",
        "        updates += 1\n",
        "  return sequence, updates\n",
        "\n",
        "def ScoreProbAverage(sequence, attempts):\n",
        "  total = 0\n",
        "  for a in attempts:\n",
        "    char = sequence[a[0]]\n",
        "    for unmask in a[1]:\n",
        "      if unmask['token_str'] == char:\n",
        "        total += unmask['score']\n",
        "  return total / len(sequence)\n",
        "\n",
        "def ScoreProbEntropy(sequence, attempts):\n",
        "  total = 0\n",
        "  for a in attempts:\n",
        "    char = sequence[a[0]]\n",
        "    for unmask in a[1]:\n",
        "      if unmask['token_str'] == char:\n",
        "        total += math.log(unmask['score'], 2) * unmask['score']\n",
        "  return total * -1\n",
        "\n",
        "\n",
        "def LogSetup(identifier, seq):\n",
        "  logfile = open(f\"logfile_{identifier}_{seq}.txt\", \"w\")\n",
        "  if use_custom_weights == \"no\":\n",
        "    logfile.write(\"No custom weights used\\n\")\n",
        "  if use_custom_weights != \"From computer\":\n",
        "    logfile.write(f'Weights from:\\t{path_to_file_in_colab}\\n')\n",
        "  else:\n",
        "    logfile.write(f'Custom weights from personal machine\\n')\n",
        "  logfile.write(f\"Original:\\t{original}\\n\")\n",
        "  logfile.write(f\"Masked:\\t{masked}\\n\\n\")\n",
        "  logfile.write(f\"Order type:\\t{order}\\n\")\n",
        "  logfile.write(f\"Deterministic:\\t{deterministic}\\n\")\n",
        "  if deterministic:\n",
        "    logfile.write(f\"Random seed:\\t{seed}\\n\")\n",
        "  # logfile.write(f\"Randomize iterations:\\t{str(randomize_every_iteration)}\\n\")\n",
        "  logfile.write(f\"Max iterations:\\t{number_of_iterations}\\n\")\n",
        "  if use_threshold:\n",
        "    logfile.write(f\"Max changes per iteration:\\t{num_changes_to_keep}\\n\")\n",
        "  else:\n",
        "    logfile.write(f\"Slowdown:\\t{slowdown}\\n\")\n",
        "  logfile.write(f\"Threshold %:\\t{keep_threshold_percent}\\n\\n\")\n",
        "  if burn_in > 0:\n",
        "    logfile.write(f\"Burn in iterations:\\t{burn_in}\\n\")\n",
        "    logfile.write(f\"Burn in max changes:\\t{burn_percent}\\n\")\n",
        "    logfile.write(f\"Burn in threshold %:\\t{burn_num_to_keep}\\n\\n\")\n",
        "  return logfile\n",
        "\n",
        "def LogSummary(logfile, iteration, original, new, data, datalog=5 ,score_avg=None, score_ent=None):\n",
        "  # original = \"\".join(data[0][1]['sequence'])\n",
        "  data.sort(key=lambda x:x[0])\n",
        "  new = \" \".join(new)\n",
        "  # print(data)\n",
        "\n",
        "  num_alpha = 0\n",
        "  for c in original:\n",
        "    if c.isalpha():\n",
        "      num_alpha += 1\n",
        "  num_str = \"\"\n",
        "  for i in range(5, len(\"\".join(original.split())) + 1, 5):\n",
        "    if i == 5:\n",
        "      num_str += f\"        {i}\"\n",
        "      continue\n",
        "    if len(str(i)) > len(str(i - 5)):\n",
        "      num_str += \" \"\n",
        "    num_str += \"\".join([\" \"] * (10 - len(str(i)))) + str(i)\n",
        "\n",
        "\n",
        "\n",
        "  change_str = \"\"\n",
        "  for i in range(len(original)):\n",
        "    if original[i] != new[i]:\n",
        "      change_str += \"*\"\n",
        "    elif original[i].isalpha() and \" \".join(masked)[i] == '_':\n",
        "      change_str += '-'\n",
        "    else:\n",
        "      change_str += \" \"\n",
        "  num_mask_str = \"\"\n",
        "  num_masked = 0\n",
        "  add_stack = []\n",
        "  for i in range(len(change_str)):\n",
        "    if change_str[i] in ['-', '*']:\n",
        "      num_masked += 1\n",
        "      if num_masked % 5 == 0:\n",
        "        num_mask_str += str(num_masked)\n",
        "        for _ in range(len(str(num_masked))):\n",
        "          add_stack.append(\"\")\n",
        "\n",
        "    if len(add_stack) != 0:\n",
        "      add_stack.pop()\n",
        "    else:\n",
        "      num_mask_str += \" \"\n",
        "\n",
        "\n",
        "  orig_spl = original.split()\n",
        "  new_spl = new.split()\n",
        "\n",
        "  logfile.write(f\"Iteration {iteration - 1}:\\n\")\n",
        "  if score_avg is not None:\n",
        "    logfile.write(f\"Score (average): {score_avg}\\n\")\n",
        "  if score_ent is not None:\n",
        "    logfile.write(f\"Score (entropy): {score_ent}\\n\")\n",
        "  logfile.write(f\"\\t       {num_str}\\n\")\n",
        "  logfile.write(f\"\\tStart: {original}\\n\")\n",
        "  logfile.write(f\"\\t       {change_str}\\n\")\n",
        "  logfile.write(f\"\\tEnd:   {new}\\n\")\n",
        "  logfile.write(f\"\\t       {num_mask_str}\\n\")\n",
        "  logfile.write(f\"\\t{change_str.count('*')} changes from previous start of iteration\\n\")\n",
        "  logfile.write(\"\\n\")\n",
        "\n",
        "  for mask in data:\n",
        "    if orig_spl[mask[0]] != new_spl[mask[0]]:\n",
        "      logfile.write(\"*\")\n",
        "    logfile.write(f\"\\tMask {mask[0] + 1}: {orig_spl[mask[0]]} -> {new_spl[mask[0]]}\\n\\t\")\n",
        "    for d in mask[1][:datalog]:\n",
        "      logfile.write(f\"\\t{d['token_str']}: {d['score']:.7f}\\t\")\n",
        "    logfile.write(\"\\n\")\n",
        "\n",
        "if MLM_model:\n",
        "  orig_batch = []\n",
        "  mask_batch = []\n",
        "  all_results = []\n",
        "\n",
        "  scores_avg = [0.0]\n",
        "  scores_ent = [0.0]\n",
        "\n",
        "\n",
        "  if not process_batch:\n",
        "    orig_batch.append(original)\n",
        "    mask_batch.append(masked)\n",
        "  else:\n",
        "    with open(path_to_batch, \"r\") as fp:\n",
        "      for line in fp.readlines():\n",
        "        orig_batch.append(line.strip())\n",
        "        mask_batch.append(\"\")\n",
        "\n",
        "\n",
        "  for i in range(len(orig_batch)):\n",
        "    print(\"\".join([\"-\"]*30))\n",
        "    original = orig_batch[i]\n",
        "    masked = mask_batch[i]\n",
        "    ###\n",
        "    assert all(c in AAs for c in original), f\"Sequence must contain only letters corresponding to the 20 amino acids\\n\\ti.e. ARNDCEQGHILKMFPSTWYV\\nProvided: {original}\"\n",
        "    if use_random_sequence:\n",
        "      original = \"\"\n",
        "      for v in range(random_sequence_length):\n",
        "        original += AAs[random.randrange(0, len(AAs))]\n",
        "      masked = \"\".join([\"_\"]*random_sequence_length)\n",
        "    #Remove possible whitespace\n",
        "    original = \"\".join(original.split())\n",
        "    if match_original:\n",
        "      masked = \"_\" * len(original)\n",
        "    assert len(original) == len(masked), f\"Original sequence and masked sequence must be the same length\\n\\tOriginal length: {len(original)}\\n\\tMasked length: {len(masked)}\"\n",
        "    ###\n",
        "\n",
        "    import time\n",
        "    t1 = time.time()\n",
        "\n",
        "    if write_log:\n",
        "      logfile = LogSetup(i, original[:3])\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "    ###preprocess###\n",
        "    new_seq = \"\"\n",
        "    col_seq = \"\"\n",
        "    #Make a clean, standardized string using _ for masks\n",
        "    for c in masked.replace('x', '_').replace(r'[MASK]', '_'):\n",
        "      if not c.isspace():\n",
        "        if spaces_in_output:\n",
        "          col_seq += (c + \" \")\n",
        "        else:\n",
        "          col_seq += c\n",
        "\n",
        "    #Make masked string according to BERT standard\n",
        "    new_seq = ' '.join(''.join(col_seq.split()))\n",
        "    orig = ' '.join(''.join(original.split()))\n",
        "\n",
        "    # print(orig)\n",
        "    print(\"Generating from: \", original.replace(' ', ''), \"\\n\", \"         using: \", new_seq.replace(' ', ''))\n",
        "    num_masked = \"\".join(new_seq).count(\"_\")\n",
        "    print(f\"Number of masked residues: {num_masked}\")\n",
        "    modify = new_seq.split()\n",
        "\n",
        "    locations = FindAllChar(modify, '_')\n",
        "    if order == 'right-to-left':\n",
        "      locations.reverse()\n",
        "    elif order == 'random':\n",
        "      random.shuffle(locations)\n",
        "\n",
        "    orig_spl = orig.split()\n",
        "    retain = orig.split()\n",
        "    before = cp.deepcopy(orig_spl)\n",
        "\n",
        "\n",
        "    iterations = []\n",
        "    iterations.append(\"\".join(orig_spl))\n",
        "    pbar = tqdm(total=(number_of_iterations * len(locations)), position=0, leave=True)\n",
        "    for _ in range(number_of_iterations):\n",
        "      data = []\n",
        "      iterations.append(\"\".join(orig_spl))\n",
        "      changes = []\n",
        "      noThreshChanges = []\n",
        "      if order == 'random':\n",
        "        random.shuffle(locations)\n",
        "      attempts = []\n",
        "      for val in locations:\n",
        "        orig_spl = cp.deepcopy(before)\n",
        "        orig_spl[val] = '[MASK]'\n",
        "\n",
        "        noThreshCont = []\n",
        "\n",
        "\n",
        "        # if get_score:\n",
        "        #   numlog = 30\n",
        "        # elif num_to_log < 5:\n",
        "        #   numlog = 5\n",
        "        # else:\n",
        "        #   numlog = num_to_log\n",
        "\n",
        "        numlog = 30\n",
        "\n",
        "        masked_seq = \" \".join(orig_spl)\n",
        "        # inputs = tokenizer(masked_seq, return_tensors='pt')\n",
        "        # inputs.to(device)\n",
        "        # unmask= unmasker(inputs, top_k=numlog)\n",
        "\n",
        "        unmask= unmasker(masked_seq, top_k=numlog)\n",
        "\n",
        "        attempts.append((val, unmask))\n",
        "\n",
        "        data.append( (val, unmask) )\n",
        "        for attempt in unmask:\n",
        "          if attempt['token'] >= 5:\n",
        "            orig_spl = before\n",
        "            changes.append((val, attempt))\n",
        "            if use_threshold:\n",
        "              break\n",
        "            else:\n",
        "              noThreshCont.append((val, attempt))\n",
        "        if not use_threshold:\n",
        "          noThreshChanges.append(noThreshCont)\n",
        "        pbar.update(1)\n",
        "      retain = cp.deepcopy(before)\n",
        "\n",
        "      score_avg = ScoreProbAverage(before, attempts)\n",
        "      scores_avg.append(score_avg)\n",
        "      if display_score_average:\n",
        "        desc_avg = f\" Current score (average): {score_avg}\"\n",
        "      else:\n",
        "        desc_avg = \"\"\n",
        "\n",
        "\n",
        "      score_ent = ScoreProbEntropy(before, attempts)\n",
        "      scores_ent.append(score_ent)\n",
        "      if display_score_entropy:\n",
        "        desc_ent = f\" Current score (entropy): {score_ent}\"\n",
        "      else:\n",
        "        desc_ent = \"\"\n",
        "      if bi > 0:\n",
        "        bi -= 1\n",
        "        if use_threshold:\n",
        "          orig_spl, num_changed = Update(changes, before, threshold=burn_percent, num_to_change=burn_num_to_keep)\n",
        "        else:\n",
        "          orig_spl, num_changed = UpdateNoThresh(noThreshChanges, before, num_to_change=burn_num_to_keep, slow=slowdown)\n",
        "      else:\n",
        "        if use_threshold:\n",
        "          orig_spl, num_changed = Update(changes, before, threshold=keep_threshold_percent, num_to_change=num_changes_to_keep)\n",
        "        else:\n",
        "          orig_spl, num_changed = UpdateNoThresh(noThreshChanges, before, num_to_change=num_changes_to_keep, slow=slowdown)\n",
        "      # orig_spl, num_changed = Update(changes, before, threshold=keep_threshold_percent, num_to_change=num_changes_to_keep)\n",
        "      description_str = f\"{num_changed} changes from previous iteration.\" + desc_avg + desc_ent\n",
        "      pbar.set_description(description_str)\n",
        "      # pbar.set_description(f\"{num_changed} changes from previous iteration. Current score{score_avg}\")\n",
        "      if write_log:\n",
        "        LogSummary(logfile, len(iterations), \"\".join(changes[0][1]['sequence']), orig_spl, data, datalog=num_to_log, score_avg=score_avg, score_ent=score_ent)\n",
        "      if ListIsEqual(retain, orig_spl) and use_threshold:\n",
        "        print(\"\\nConverged\")\n",
        "        all_results.append(\"\".join(orig_spl))\n",
        "        break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    iterations.append(\"\".join(orig_spl))\n",
        "    if write_log:\n",
        "      print(f\"Progress and results dumped to logfile_{i}.txt\")\n",
        "      logfile.close()\n",
        "\n",
        "    if not ListIsEqual(retain, orig_spl):\n",
        "      print(f\"\\nNo convergence in {number_of_iterations} iterations\")\n",
        "\n",
        "\n",
        "    if spaces_in_output:\n",
        "      # print(\"Original: \", \" \".join(retain))\n",
        "      print(\"\\nFinal Result:\\n\", \" \".join(orig_spl))\n",
        "    else:\n",
        "      # print(\"Original: \", \"\".join(retain))\n",
        "      print(\"\\nFinal Result:\\n\", \"\".join(orig_spl))\n",
        "\n",
        "    print(\"Time: \", time.time() - t1)\n",
        "\n",
        "\n",
        "    def PrintColoredDifferences(l1, l2, color='red', fin = \"\", score_avg=None, score_ent=None):\n",
        "      length = len(l1)\n",
        "      num_diff = 0\n",
        "      num_c = []\n",
        "      assert len(l1) == len(l2), \"DifferentSize error\"\n",
        "      for i in range(len(l1)):\n",
        "        if l1[i] == l2[i]:\n",
        "          print(f\"{l2[i]}\", end = fin)\n",
        "        else:\n",
        "          print(colored(f\"{l2[i]}\", 'red'), end = fin)\n",
        "          num_diff += 1\n",
        "      print(\" | \", end=\"\")\n",
        "      if display_num_different:\n",
        "        print(f\" {num_diff:^{len(' Changes |') - 3}}\", end= \"|\")\n",
        "      if display_score_average:\n",
        "        print(f\" {score_avg:^{len('| Avg. Score |') - 3}.4f}\", end=\"|\")\n",
        "      if display_score_entropy:\n",
        "        print(f\" {score_ent:^{len('| Avg. Score |') - 3}.4f}\", end=\"|\")\n",
        "\n",
        "      print()\n",
        "      return num_diff\n",
        "\n",
        "\n",
        "    if display_iterations:\n",
        "      print(\"Iterations:\")\n",
        "      num_c = []\n",
        "      print(f\"{'Sequence':^{1 + len(iterations[0])}}\", end = \"|\")\n",
        "      if display_num_different:\n",
        "        print(\" Changes |\", end = \"\")\n",
        "      if display_score_average:\n",
        "        print(\" Avg. Score |\", end = \"\")\n",
        "      if display_score_entropy:\n",
        "        print(\" Ent. Score |\", end = \"\")\n",
        "      print()\n",
        "      for i in range(1, len(iterations)):\n",
        "        if spaces_in_output:\n",
        "          num_c.append(PrintColoredDifferences(iterations[i - 1], iterations[i], 'red', \" \", score_avg=scores_avg[i - 1], score_ent=scores_ent[i - 1]))\n",
        "        else:\n",
        "          num_c.append(PrintColoredDifferences(iterations[i - 1], iterations[i], 'red', \"\", score_avg=scores_avg[i - 1], score_ent=scores_ent[i - 1]))\n",
        "        if display_score_average:\n",
        "          pass\n",
        "        plt.plot(num_c[1:])\n",
        "        plt.title(\"Number of Changes per Iteration\")\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Number of Changes')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    if display_score_average:\n",
        "      plt.plot(scores_avg)\n",
        "      plt.title(\"Scores over each iteration (average)\")\n",
        "      plt.xlabel('Iteration')\n",
        "      plt.ylabel('Score')\n",
        "      plt.show()\n",
        "\n",
        "    if display_score_entropy:\n",
        "      plt.plot(scores_ent)\n",
        "      plt.title(\"Scores over each iteration (entropy)\")\n",
        "      plt.xlabel('Iteration')\n",
        "      plt.ylabel('Score')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "  # print(\"\".join([\"-\"]*30))\n",
        "  # print(\"Summary:\")\n",
        "  # for r in all_results:\n",
        "  #   print(r) Okay, so I think I see the problem. Go to line 411 and change that \"iterations[i - 1]\" to \"iterations[0]\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}